# uncertainty_gbm
Sklearn implementation of GBM to predict mu(X) and std(X) on heteroscedastic
data.

The traditional application of GBM is to predict y_hat(X) using an additive
ensemble of decision trees, minimizing squared error loss on known true
observations y(X).

In uncertainty-GBM, we assume the data is heteroscedastic, meaning that the
variance of the observed values y(X) is not constant everywhere, but rather is
described by some other function.  That is, we assume our data is generated
via some functions mu, std: given X, the targets y are generated by taking a
sample from a normal distribution with mean mu(X) and standard deviation
std(X).  Thus, the GBM predicts both mu_hat(X) and std_hat(X) to minimize
negative log-likelihood (NLL).

See dummy_demo.py for a demonstration of the model on constructed data.
